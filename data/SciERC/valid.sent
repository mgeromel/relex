This paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination .
The problem is formulated in a manner that subsumes structure from motion , multi-view stereo , and photo-metric stereo as special cases .
The algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .
The algorithm works by iteratively estimating affine camera parameters , illumination , shape , and albedo in an alternating fashion .
An entity-oriented approach to restricted-domain parsing is proposed .
Like semantic grammar , this allows easy exploitation of limited domain semantics .
In addition , it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input .
Representative samples from an entity-oriented language definition are presented , along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses .
A parser incorporating the control structure and the parsing strategies is currently under implementation .
This paper summarizes the formalism of Category Cooccurrence Restrictions -LRB- CCRs -RRB- and describes two parsing algorithms that interpret it .
The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements .
The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism .
Special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees .
We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora .
This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author 's text as a coherent corpus .
Our proposed method improves the accuracy of our term aggregation system , showing that our approach is successful .
In this work , we present a technique for robust estimation , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more efficient robust estimation algorithm .
The combination of these two strategies results in a robust estimation procedure that provides a significant speed-up over existing RANSAC techniques , while requiring no prior information to guide the sampling process .
In particular , our algorithm requires , on average , 3-10 times fewer samples than standard RANSAC , which is in close agreement with theoretical predictions .
The efficiency of the algorithm is demonstrated on a selection of geometric estimation problems .
An attempt has been made to use an Augmented Transition Network as a procedural dialog model .
The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .
A standard ATN should be further developed in order to account for the verbal interactions of task-oriented dialogs .
We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines .
The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train an svm to separate the two classes .
We show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and news articles from trec , and that it helps the search engine handle definition questions significantly better .
We revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective .
In the case of known expert competence levels , we give sharp error estimates for the optimal rule .
We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph .
We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges .
Finally , we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph .
We apply a decision tree based approach to pronoun resolution in spoken dialogue .
Our system deals with pronouns with NP - and non-NP-antecedents .
We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features .
We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron 's -LRB- 2002 -RRB- manually tuned system .
We present a new approach for building an efficient and robust classifier for the two class problem , that localizes objects that may appear in the image under different orien-tations .
In contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step approach with an estimation stage and a classification stage .
The estimator yields an initial set of potential object poses that are then validated by the classifier .
This methodology allows reducing the time complexity of the algorithm while classification results remain high .
The classifier we use in both stages is based on a boosted combination of Random Ferns over local histograms of oriented gradients -LRB- HOGs -RRB- , which we compute during a pre-processing step .
Both the use of supervised learning and working on the gradient space makes our approach robust while being efficient at run-time .
We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as cluttered backgrounds , changing illumination conditions and partial occlusions .
A very simple improved duration model has reduced the error rate by about 10 % in both triphone and semiphone systems .
A new training strategy has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique .
Finally , the recognizer has been modified to use bigram back-off language models .
The system was then transferred from the RM task to the ATIS CSR task and a limited number of development tests performed .
A new approach for Interactive Machine Translation where the author interacts during the creation or the modification of the document is proposed .
This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser 's multiple output .
We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .
This method requires a source-language dependency parser , target language word segmentation and an unsupervised word alignment component .
We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser .
Video provides not only rich visual cues such as motion and appearance , but also much less explored long-range temporal interactions among objects .
We aim to capture such interactions and to construct a powerful intermediate-level video representation for subsequent recognition .
First , we develop an efficient spatio-temporal video segmentation algorithm , which naturally incorporates long-range motion cues from the past and future frames in the form of clusters of point tracks with coherent motion .
Second , we devise a new track clustering cost function that includes occlusion reasoning , in the form of depth ordering constraints , as well as motion similarity along the tracks .
We evaluate the proposed approach on a challenging set of video sequences of office scenes from feature length movies .
In this paper , we introduce KAZE features , a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces .
In contrast , we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering .
The nonlinear scale space is built using efficient Additive Operator Splitting -LRB- AOS -RRB- techniques and variable con-ductance diffusion .
Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods .
Creating summaries on lengthy Semantic Web documents for quick identification of the corresponding entity has been of great contemporary interest .
Specifically , we highlight the importance of diversified -LRB- faceted -RRB- summaries by combining three dimensions : diversity , uniqueness , and popularity .
Our novel diversity-aware entity summarization approach mimics human conceptual clustering techniques to group facts , and picks representative facts from each group to form concise -LRB- i.e. , short -RRB- and comprehensive -LRB- i.e. , improved coverage through diversity -RRB- summaries .
We evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of entity summarization .
We present a framework for the fast computation of lexical affinity models .
The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a parametric affinity model .
In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .
We apply it in combination with a terabyte corpus to answer natural language tests , achieving encouraging results .
This paper introduces a system for categorizing unknown words .
The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words .
The focus of this paper is the components that identify names and spelling errors .
Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word .
The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words .
At MIT Lincoln Laboratory , we have been developing a Korean-to-English machine translation system CCLINC -LRB- Common Coalition Language System at Lincoln Laboratory -RRB- .
The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame .
The key features of the system include : -LRB- i -RRB- Robust efficient parsing of Korean -LRB- a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments -RRB- .
-LRB- ii -RRB- High quality translation via word sense disambiguation and accurate word order generation of the target language .
Having been trained on Korean newspaper articles on missiles and chemical biological warfare , the system produces the translation output sufficient for content understanding of the original document .
The JAVELIN system integrates a flexible , planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text .
We present the first application of the head-driven statistical parsing model of Collins -LRB- 1999 -RRB- as a simultaneous language model and parser for large-vocabulary speech recognition .
The model is adapted to an online left to right chart-parser for word lattices , integrating acoustic , n-gram , and parser probabilities .
The parser uses structural and lexical dependencies not considered by n-gram models , conditioning recognition on more linguistically-grounded relationships .
Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .
Image composition -LRB- or mosaicing -RRB- has attracted a growing attention in recent years as one of the main elements in video analysis and representation .
In this paper we deal with the problem of global alignment and super-resolution .
We also propose to evaluate the quality of the resulting mosaic by measuring the amount of blurring .
Global registration is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps .
Experimental comparison with other techniques shows the effectiveness of our approach .
The main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish .
We present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , feasibility study for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description .
The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish .
Along with the increasing requirements , the hash-tag recommendation task for microblogs has been receiving considerable attention in recent years .
Motivated by the successful use of convolutional neural networks -LRB- CNNs -RRB- for many natural language processing tasks , in this paper , we adopt CNNs to perform the hashtag recommendation problem .
To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works , we propose a novel architecture with an attention mechanism .
The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods .
By incorporating trigger words into the consideration , the relative improvement of the proposed method over the state-of-the-art method is around 9.4 % in the F1-score .
In this paper , we improve an unsupervised learning method using the Expectation-Maximization -LRB- EM -RRB- algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation -LRB- WSD -RRB- problems .
In experiments , we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2 .
Furthermore , our methods were confirmed to be effective also for verb WSD problems .
Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval .
-LRB- Ramshaw and Marcus , 1995 -RRB- have introduced a `` convenient '' data representation for chunking by converting it to a tagging task .
In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks .
However , equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web .
In this paper we evaluate four objective measures of speech with regards to intelligibility prediction of synthesized speech in diverse noisy situations .
We evaluated three intel-ligibility measures , the Dau measure , the glimpse proportion and the Speech Intelligibility Index -LRB- SII -RRB- and a quality measure , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- .
For the generation of synthesized speech we used a state of the art HMM-based speech synthesis system .
The noisy conditions comprised four additive noises .
The measures were compared with subjective intelligibility scores obtained in listening tests .
The results show the Dau and the glimpse measures to be the best predictors of intelligibility , with correlations of around 0.83 to subjective scores .
All measures gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech ; in particular the SII measure .
In additional experiments , we processed the synthesized speech by an ideal binary mask before adding noise .
The Glimpse measure gave the most accurate intelligibility predictions in this situation .
A '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : reconstruction on demand by tensor voting , or ROD-TV .
ROD-TV simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -LRB- LOD -RRB- .
Locally inferred surface elements are robust to noise and better capture local shapes .
By inferring per-vertex normals at sub-voxel precision on the fly , we can achieve interpolative shading .
By relaxing the mesh connectivity requirement , we extend ROD-TV and propose a simple but effective multiscale feature extraction algorithm .
ROD-TV consists of a hierarchical data structure that encodes different levels of detail .
The local reconstruction algorithm is tensor voting .
It is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and collecting tensorial support in a neighborhood .
Both rhetorical structure and punctuation have been helpful in discourse processing .
Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , Question Mark , and Semicolon .
The rhetorical patterns of these marks are compared against patterns around cue phrases in general .
Results show that these Chinese punctuation marks , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in Chinese texts .
The features based on Markov random field -LRB- MRF -RRB- models are usually sensitive to the rotation of image textures .
This paper develops an anisotropic circular Gaussian MRF -LRB- ACGMRF -RRB- model for modelling rotated image textures and retrieving rotation-invariant texture features .
To overcome the singularity problem of the least squares estimate -LRB- LSE -RRB- method , an approximate least squares estimate -LRB- ALSE -RRB- method is proposed to estimate the parameters of the ACGMRF model .
The rotation-invariant features can be obtained from the parameters of the ACGMRF model by the one-dimensional -LRB- 1-D -RRB- discrete Fourier transform -LRB- DFT -RRB- .
Significantly improved accuracy can be achieved by applying the rotation-invariant features to classify SAR -LRB- synthetic aperture radar -RRB- sea ice and Brodatz imagery .
Despite much recent progress on accurate semantic role labeling , previous work has largely used independent classifiers , possibly combined with separate label sequence models via Viterbi decoding .
We show how to build a joint model of argument frames , incorporating novel features that model these interactions into discriminative log-linear models .
This system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank .
In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse .
Test performance data will show that a PCFG yields good results in morphological parsing .
MORPA is a fully implemented parser developed for use in a text-to-speech conversion system .
This paper describes the framework of a Korean phonological knowledge base system using the unification-based grammar formalism : Korean Phonology Structure Grammar -LRB- KPSG -RRB- .
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system .
We show that the proposed approach is more describable than other approaches such as those employing a traditional generative phonological approach .
In this paper , we study the design of core-selecting payment rules for such domains .
We design two core-selecting rules that always satisfy IR in expectation .
To study the performance of our rules we perform a computational Bayes-Nash equilibrium analysis .
We show that , in equilibrium , our new rules have better incentives , higher efficiency , and a lower rate of ex-post IR violations than standard core-selecting rules .
In this paper , we will describe a search tool for a huge set of ngrams .
This system can be a very useful tool for linguistic knowledge discovery and other NLP tasks .
This paper explores the role of user modeling in such systems .
Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic .
Next , the benefits and costs of implementing a user modeling component for a system are weighed in light of several aspects of the interaction requirements that may be imposed by the system .
Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents .
Despite the successes of these systems , accuracy will always be imperfect .
The information extraction system we evaluate is based on a linear-chain conditional random field -LRB- CRF -RRB- , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model .
We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .
In this paper , we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries .
We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy , focusing on noun phrases .
In this paper , we propose a new approach to generate oriented object proposals -LRB- OOPs -RRB- to reduce the detection error caused by various orientations of the object .
To this end , we propose to efficiently locate object regions according to pixelwise object probability , rather than measuring the objectness from a set of sampled windows .
We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes -LRB- i.e. , sizes and orientations -RRB- can be produced by locating the local maximum likelihoods .
First , it helps the object detector handle objects of different orientations .
Third , it avoids massive window sampling , and thereby reducing the number of proposals while maintaining a high recall .
Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods .
Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios .
This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase .
Finally , we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving OCR accuracy .
We investigate the problem of fine-grained sketch-based image retrieval -LRB- SBIR -RRB- , where free-hand human sketches are used as queries to perform instance-level retrieval of images .
This is an extremely challenging task because -LRB- i -RRB- visual comparisons not only need to be fine-grained but also executed cross-domain , -LRB- ii -RRB- free-hand -LRB- finger -RRB- sketches are highly abstract , making fine-grained matching harder , and most importantly -LRB- iii -RRB- annotated cross-domain sketch-photo datasets required for training are scarce , challenging many state-of-the-art machine learning techniques .
We then develop a deep triplet-ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data .
Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks .
In this paper we target at generating generic action proposals in unconstrained videos .
Each action proposal corresponds to a temporal series of spatial bounding boxes , i.e. , a spatio-temporal video tube , which has a good potential to locate one human action .
Assuming each action is performed by a human with meaningful motion , both appearance and motion cues are utilized to measure the ac-tionness of the video tubes .
After picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score .
Compared with existing action proposal approaches , our action proposals do not rely on video segmentation and can be generated in nearly real-time .
Experimental results on two challenging datasets , MSRII and UCF 101 , validate the superior performance of our action proposals as well as competitive results on action detection and search .
This paper reports recent research into methods for creating natural language text .
KDS -LRB- Knowledge Delivery System -RRB- , which embodies this paradigm , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among competing combinations , and to creation of the final text .
The Fragment-and-Compose paradigm and the computational methods of KDS are described .
This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful .
The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms .
Preliminary experiments with similarities computed using first-order and second-order co-occurrence seem to confirm the hypothesis .
We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several , previously proposed phrase-based translation models .
Within our framework , we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models .
Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations .
Traditional methods for color constancy can improve surface re-flectance estimates from such uncalibrated images , but their output depends significantly on the background scene .
We introduce the multi-view color constancy problem , and present a method to recover estimates of underlying surface re-flectance based on joint estimation of these surface properties and the illuminants present in multiple images .
The method can exploit image correspondences obtained by various alignment techniques , and we show examples based on matching local region features .
Our results show that multi-view constraints can significantly improve estimates of both scene illuminants and object color -LRB- surface reflectance -RRB- when compared to a baseline single-view method .
Our contributions include a concise , modular architecture with reversible processes of understanding and generation , an information-state model of reference , and flexible links between semantics and collaborative problem solving .
